{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRUEKR/kIiAPrY4zB/IJGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshaneeb11/word_embedding/blob/main/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Frsv-VUWOHh",
        "outputId": "ab7dc7cd-c2ea-468a-aa4c-cf7c4c049e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp312-cp312-linux_x86_64.whl size=4498212 sha256=89bf414d63088fb3e596b7bae0a2a244f136c1797742033408e135b6b5178d60\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/27/95/a7baf1b435f1cbde017cabdf1e9688526d2b0e929255a359c6\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "help(Word2Vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hYLtMfc6WpTR",
        "outputId": "9f152ff6-a1ef-4b0e-d9a7-3e88a9491a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class Word2Vec in module gensim.models.word2vec:\n",
            "\n",
            "class Word2Vec(gensim.utils.SaveLoad)\n",
            " |  Word2Vec(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      Word2Vec\n",
            " |      gensim.utils.SaveLoad\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
            " |      Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
            " |\n",
            " |      Once you're finished training a model (=no more updates, only querying)\n",
            " |      store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
            " |      to reduce memory.\n",
            " |\n",
            " |      The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
            " |      :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
            " |\n",
            " |      The trained word vectors can also be stored/loaded from a format compatible with the\n",
            " |      original word2vec implementation via `self.wv.save_word2vec_format`\n",
            " |      and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of iterables, optional\n",
            " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |          See also the `tutorial on data streaming in Python\n",
            " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
            " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
            " |          in some other way.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
            " |      vector_size : int, optional\n",
            " |          Dimensionality of the word vectors.\n",
            " |      window : int, optional\n",
            " |          Maximum distance between the current and predicted word within a sentence.\n",
            " |      min_count : int, optional\n",
            " |          Ignores all words with total frequency lower than this.\n",
            " |      workers : int, optional\n",
            " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
            " |      sg : {0, 1}, optional\n",
            " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
            " |      hs : {0, 1}, optional\n",
            " |          If 1, hierarchical softmax will be used for model training.\n",
            " |          If 0, hierarchical softmax will not be used for model training.\n",
            " |      negative : int, optional\n",
            " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
            " |          should be drawn (usually between 5-20).\n",
            " |          If 0, negative sampling will not be used.\n",
            " |      ns_exponent : float, optional\n",
            " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
            " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
            " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
            " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
            " |          other values may perform better for recommendation applications.\n",
            " |      cbow_mean : {0, 1}, optional\n",
            " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
            " |      alpha : float, optional\n",
            " |          The initial learning rate.\n",
            " |      min_alpha : float, optional\n",
            " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
            " |      seed : int, optional\n",
            " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
            " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
            " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
            " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
            " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
            " |      max_vocab_size : int, optional\n",
            " |          Limits the RAM during vocabulary building; if there are more unique\n",
            " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
            " |          Set to `None` for no limit.\n",
            " |      max_final_vocab : int, optional\n",
            " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
            " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
            " |          Set to `None` if not required.\n",
            " |      sample : float, optional\n",
            " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
            " |          useful range is (0, 1e-5).\n",
            " |      hashfxn : function, optional\n",
            " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
            " |      epochs : int, optional\n",
            " |          Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
            " |          model.\n",
            " |\n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |      sorted_vocab : {0, 1}, optional\n",
            " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
            " |          See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
            " |      batch_words : int, optional\n",
            " |          Target size (in words) for batches of examples passed to worker threads (and\n",
            " |          thus cython routines).(Larger batches will be passed if individual\n",
            " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
            " |      compute_loss: bool, optional\n",
            " |          If True, computes and stores loss value which can be retrieved using\n",
            " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
            " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
            " |          Sequence of callbacks to be executed at specific stages during training.\n",
            " |      shrink_windows : bool, optional\n",
            " |          New in 4.1. Experimental.\n",
            " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
            " |          for each target word during training, to match the original word2vec algorithm's\n",
            " |          approximate weighting of context words by distance. Otherwise, the effective\n",
            " |          window size is always fixed to `window` words to either side.\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
            " |\n",
            " |      .. sourcecode:: pycon\n",
            " |\n",
            " |          >>> from gensim.models import Word2Vec\n",
            " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |          >>> model = Word2Vec(sentences, min_count=1)\n",
            " |\n",
            " |      Attributes\n",
            " |      ----------\n",
            " |      wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
            " |          This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
            " |          directly to query those embeddings in various ways. See the module level docstring for examples.\n",
            " |\n",
            " |  __str__(self)\n",
            " |      Human readable representation of the model's state.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      str\n",
            " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
            " |          and learning rate.\n",
            " |\n",
            " |  add_null_word(self)\n",
            " |\n",
            " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
            " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      corpus_iterable : iterable of list of str\n",
            " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (not both of them).\n",
            " |      update : bool\n",
            " |          If true, the new words in `sentences` will be added to model's vocab.\n",
            " |      progress_per : int, optional\n",
            " |          Indicates how many words to process before showing/updating the progress.\n",
            " |      keep_raw_vocab : bool, optional\n",
            " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
            " |          of the model.\n",
            " |\n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |\n",
            " |      **kwargs : object\n",
            " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
            " |\n",
            " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
            " |      Build vocabulary from a dictionary of word frequencies.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      word_freq : dict of (str, int)\n",
            " |          A mapping from a word in the vocabulary to its frequency count.\n",
            " |      keep_raw_vocab : bool, optional\n",
            " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
            " |      corpus_count : int, optional\n",
            " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
            " |      trim_rule : function, optional\n",
            " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
            " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
            " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
            " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
            " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
            " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
            " |          of the model.\n",
            " |\n",
            " |          The input parameters are of the following types:\n",
            " |              * `word` (str) - the word we are examining\n",
            " |              * `count` (int) - the word's frequency count in the corpus\n",
            " |              * `min_count` (int) - the minimum count threshold.\n",
            " |\n",
            " |      update : bool, optional\n",
            " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
            " |\n",
            " |  create_binary_tree(self)\n",
            " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
            " |      word counts. Frequent words will have shorter binary codes.\n",
            " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
            " |\n",
            " |  estimate_memory(self, vocab_size=None, report=None)\n",
            " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      vocab_size : int, optional\n",
            " |          Number of unique tokens in the vocabulary\n",
            " |      report : dict of (str, int), optional\n",
            " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      dict of (str, int)\n",
            " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
            " |\n",
            " |  get_latest_training_loss(self)\n",
            " |      Get current value of the training loss.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      float\n",
            " |          Current training loss.\n",
            " |\n",
            " |  init_sims(self, replace=False)\n",
            " |      Precompute L2-normalized vectors. Obsoleted.\n",
            " |\n",
            " |      If you need a single unit-normalized vector for some key, call\n",
            " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
            " |      ``word2vec_model.wv.get_vector(key, norm=True)``.\n",
            " |\n",
            " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
            " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      replace : bool\n",
            " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
            " |          You lose information if you do this.\n",
            " |\n",
            " |  init_weights(self)\n",
            " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
            " |\n",
            " |  make_cum_table(self, domain=2147483647)\n",
            " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
            " |      drawing random words in the negative-sampling training routines.\n",
            " |\n",
            " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
            " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
            " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
            " |\n",
            " |  predict_output_word(self, context_words_list, topn=10)\n",
            " |      Get the probability distribution of the center word given context words.\n",
            " |\n",
            " |      Note this performs a CBOW-style propagation, even in SG models,\n",
            " |      and doesn't quite weight the surrounding words the same as in\n",
            " |      training -- so it's just one crude way of using a trained model\n",
            " |      as a predictor.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      context_words_list : list of (str and/or int)\n",
            " |          List of context words, which may be words themselves (str)\n",
            " |          or their index in `self.wv.vectors` (int).\n",
            " |      topn : int, optional\n",
            " |          Return `topn` words and their probabilities.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      list of (str, float)\n",
            " |          `topn` length list of tuples of (word, probability).\n",
            " |\n",
            " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
            " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
            " |      and `sample` (controlling the downsampling of more-frequent words).\n",
            " |\n",
            " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
            " |      report the size of the retained vocabulary, effective corpus length, and\n",
            " |      estimated memory requirements. Results are both printed via logging and\n",
            " |      returned as a dict.\n",
            " |\n",
            " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
            " |      unless `keep_raw_vocab` is set.\n",
            " |\n",
            " |  prepare_weights(self, update=False)\n",
            " |      Build tables and model weights based on final vocabulary settings.\n",
            " |\n",
            " |  reset_from(self, other_model)\n",
            " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
            " |\n",
            " |      Structures copied are:\n",
            " |          * Vocabulary\n",
            " |          * Index to word mapping\n",
            " |          * Cumulative frequency table (used for negative sampling)\n",
            " |          * Cached corpus length\n",
            " |\n",
            " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
            " |      then share all vocabulary-related structures other than vectors, neither should then\n",
            " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
            " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
            " |\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
            " |          Another model to copy the internal structures from.\n",
            " |\n",
            " |  save(self, *args, **kwargs)\n",
            " |      Save the model.\n",
            " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
            " |      online training and getting vectors for vocabulary words.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to the file.\n",
            " |\n",
            " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
            " |\n",
            " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
            " |      Score the log probability for a sequence of sentences.\n",
            " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
            " |\n",
            " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
            " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
            " |\n",
            " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
            " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
            " |\n",
            " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
            " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
            " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
            " |      how to use such scores in document classification.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sentences : iterable of list of str\n",
            " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |      total_sentences : int, optional\n",
            " |          Count of sentences.\n",
            " |      chunksize : int, optional\n",
            " |          Chunksize of jobs\n",
            " |      queue_factor : int, optional\n",
            " |          Multiplier for size of queue (number of workers * queue_factor).\n",
            " |      report_delay : float, optional\n",
            " |          Seconds to wait before reporting progress.\n",
            " |\n",
            " |  seeded_vector(self, seed_string, vector_size)\n",
            " |\n",
            " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
            " |      Update the model's neural weights from a sequence of sentences.\n",
            " |\n",
            " |      Notes\n",
            " |      -----\n",
            " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
            " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
            " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
            " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
            " |      you can simply use `total_examples=self.corpus_count`.\n",
            " |\n",
            " |      Warnings\n",
            " |      --------\n",
            " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
            " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
            " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      corpus_iterable : iterable of list of str\n",
            " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
            " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
            " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
            " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
            " |          See also the `tutorial on data streaming in Python\n",
            " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
            " |      corpus_file : str, optional\n",
            " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
            " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
            " |          `corpus_file` arguments need to be passed (not both of them).\n",
            " |      total_examples : int\n",
            " |          Count of sentences.\n",
            " |      total_words : int\n",
            " |          Count of raw words in sentences.\n",
            " |      epochs : int\n",
            " |          Number of iterations (epochs) over the corpus.\n",
            " |      start_alpha : float, optional\n",
            " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
            " |          for this one call to`train()`.\n",
            " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
            " |          (not recommended).\n",
            " |      end_alpha : float, optional\n",
            " |          Final learning rate. Drops linearly from `start_alpha`.\n",
            " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
            " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
            " |          (not recommended).\n",
            " |      word_count : int, optional\n",
            " |          Count of words already trained. Set this to 0 for the usual\n",
            " |          case of training on all words in sentences.\n",
            " |      queue_factor : int, optional\n",
            " |          Multiplier for size of queue (number of workers * queue_factor).\n",
            " |      report_delay : float, optional\n",
            " |          Seconds to wait before reporting progress.\n",
            " |      compute_loss: bool, optional\n",
            " |          If True, computes and stores loss value which can be retrieved using\n",
            " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
            " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
            " |          Sequence of callbacks to be executed at specific stages during training.\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      .. sourcecode:: pycon\n",
            " |\n",
            " |          >>> from gensim.models import Word2Vec\n",
            " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
            " |          >>>\n",
            " |          >>> model = Word2Vec(min_count=1)\n",
            " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
            " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
            " |          (1, 30)\n",
            " |\n",
            " |  update_weights(self)\n",
            " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |\n",
            " |  load(*args, rethrow=False, **kwargs)\n",
            " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
            " |\n",
            " |      See Also\n",
            " |      --------\n",
            " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
            " |          Save model.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      fname : str\n",
            " |          Path to the saved file.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
            " |          Loaded model.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from gensim.utils.SaveLoad:\n",
            " |\n",
            " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
            " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
            " |      optionally log the event at `log_level`.\n",
            " |\n",
            " |      Events are important moments during the object's life, such as \"model created\",\n",
            " |      \"model saved\", \"model loaded\", etc.\n",
            " |\n",
            " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
            " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
            " |      but is useful during debugging and support.\n",
            " |\n",
            " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
            " |      will not record events into `self.lifecycle_events` then.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      event_name : str\n",
            " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
            " |      event : dict\n",
            " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
            " |          Can be empty.\n",
            " |\n",
            " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
            " |\n",
            " |          - `datetime`: the current date & time\n",
            " |          - `gensim`: the current Gensim version\n",
            " |          - `python`: the current Python version\n",
            " |          - `platform`: the current platform\n",
            " |          - `event`: the name of this event\n",
            " |      log_level : int\n",
            " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import brown\n",
        "import nltk\n",
        "nltk.download(\"brown\")\n",
        "\n",
        "#Load sentences from the Brown corpus\n",
        "sentences = brown.sents()\n",
        "\n",
        "# Train a Word2Vec model with hyperparameter\n",
        "model = Word2Vec(sentences, vector_size=300,window=5, min_count=10, sg=0, epochs=50)\n",
        "\n",
        "# Find the similar words\n",
        "similar_words = model.wv.most_similar(\"man\", topn=5)\n",
        "print(\"Similar words to 'man':\")\n",
        "for word, score in similar_words:\n",
        "  print(f\"{word}: {score:4f}\")\n",
        "\n",
        "# Perform word analogy\n",
        "analogy_result = model.wv.most_similar(positive=[\"king\",\"woman\"],negative=[\"man\"], topn=1)\n",
        "print(\"\\nWord analogy 'king - man + woman':\")\n",
        "for word, score in analogy_result:\n",
        "  print(f\"{word}: {score: 4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znKlk-YUXyux",
        "outputId": "d0ba59d1-e887-4f09-d594-08531a5f0ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to 'man':\n",
            "girl: 0.440071\n",
            "woman: 0.433808\n",
            "person: 0.403480\n",
            "guy: 0.400508\n",
            "writer: 0.398072\n",
            "\n",
            "Word analogy 'king - man + woman':\n",
            "singing:  0.343159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYuaWJ9deq1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}